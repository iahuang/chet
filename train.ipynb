{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33338144\n"
     ]
    }
   ],
   "source": [
    "from chet import Chet34, tokenize_board\n",
    "\n",
    "model = Chet34()\n",
    "model.to(device)\n",
    "print(model.get_n_params())\n",
    "\n",
    "tokenizer = lambda board: tokenize_board(board).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import chess\n",
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    csv_file: str,\n",
    "    tokenizer: Callable[[chess.Board], torch.Tensor],\n",
    "    *,\n",
    "    limit: int | None = None,\n",
    "    skip_header: bool = True,\n",
    ") -> \"ChessDataset\":\n",
    "    \"\"\"\n",
    "    Load a dataset from a CSV file.\n",
    "\n",
    "    Expected format:\n",
    "    ```\n",
    "    board_fen,move_uci\n",
    "    rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1,e2e4\n",
    "    ...\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    boards = []\n",
    "    moves = []\n",
    "\n",
    "    with open(csv_file, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        if skip_header:\n",
    "            next(reader)\n",
    "\n",
    "        for row in tqdm(reader, desc=\"Loading dataset\", total=limit):\n",
    "            board_fen, move_uci = row\n",
    "            board = board_fen\n",
    "            boards.append(board)\n",
    "\n",
    "            move = move_uci\n",
    "            moves.append(move)\n",
    "\n",
    "            if limit and len(moves) == limit:\n",
    "                break\n",
    "\n",
    "    return ChessDataset(boards, moves, tokenizer)\n",
    "\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    boards: list[str]\n",
    "    moves: list[str]\n",
    "    tokenizer: Callable[[chess.Board], torch.Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        boards: list[str],\n",
    "        moves: list[str],\n",
    "        tokenizer: Callable[[chess.Board], torch.Tensor],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.boards = boards\n",
    "        self.moves = moves\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.moves)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a training example from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the example to get\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: Tuple containing:\n",
    "                - Board tokens tensor of shape [65]\n",
    "                - Target move probabilities tensor of shape [4096]\n",
    "                - Legal move mask tensor of shape [4096]. 1.0 if the move is legal, 0.0 otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        board = chess.Board(self.boards[idx])\n",
    "        move = chess.Move.from_uci(self.moves[idx])\n",
    "\n",
    "        board_tokens = self.tokenizer(board)\n",
    "        target = torch.zeros(4096)\n",
    "\n",
    "        target[move.from_square * 64 + move.to_square] = 1.0\n",
    "\n",
    "        legal_move_mask = torch.zeros(4096, dtype=torch.bool)\n",
    "        for move in board.legal_moves:\n",
    "            legal_move_mask[move.from_square * 64 + move.to_square] = True\n",
    "\n",
    "        assert legal_move_mask.sum() > 0, \"No legal moves found\"\n",
    "        assert target.sum() == 1.0, \"Target is not a one-hot vector\"\n",
    "        assert (\n",
    "            legal_move_mask * target\n",
    "        ).sum() > 0, f\"Target is not in the legal move mask. FEN: {board.fen()} MOVE: {self.moves[idx]}\"\n",
    "\n",
    "        return board_tokens, target, legal_move_mask\n",
    "\n",
    "\n",
    "def split_dataset(\n",
    "    dataset: ChessDataset, val_split: float\n",
    ") -> tuple[ChessDataset, ChessDataset]:\n",
    "    n_val = int(len(dataset) * val_split)\n",
    "    train_boards = dataset.boards[:-n_val]\n",
    "    val_boards = dataset.boards[-n_val:]\n",
    "\n",
    "    train_moves = dataset.moves[:-n_val]\n",
    "    val_moves = dataset.moves[-n_val:]\n",
    "\n",
    "    return (\n",
    "        ChessDataset(train_boards, train_moves, dataset.tokenizer),\n",
    "        ChessDataset(val_boards, val_moves, dataset.tokenizer),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|█████████▉| 999/1000 [00:00<00:00, 767841.25it/s]\n"
     ]
    }
   ],
   "source": [
    "all_dataset = load_dataset(\"data_processing/training_data_2.csv\", tokenizer, limit=1000)\n",
    "\n",
    "train_dataset, val_dataset = split_dataset(all_dataset, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    batch_size: int\n",
    "    lr: float\n",
    "    weight_decay: float\n",
    "    warmup_steps: int\n",
    "\n",
    "    checkpoint_path: str\n",
    "    checkpoint_every: int\n",
    "\n",
    "    training_loss_last_n_batches: int\n",
    "    metrics_path: str\n",
    "\n",
    "    device: torch.device\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    loss: float\n",
    "    accuracy: float\n",
    "\n",
    "\n",
    "class TrainingHistory:\n",
    "    train: list[TrainingMetrics]\n",
    "    val: list[TrainingMetrics]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train = []\n",
    "        self.val = []\n",
    "\n",
    "    def add_point(self, train: TrainingMetrics, val: TrainingMetrics):\n",
    "        self.train.append(train)\n",
    "        self.val.append(val)\n",
    "\n",
    "    def get_average(self) -> TrainingMetrics:\n",
    "        return TrainingMetrics(\n",
    "            sum(m.loss for m in self.train) / len(self.train),\n",
    "            sum(m.accuracy for m in self.train) / len(self.train),\n",
    "        )\n",
    "\n",
    "    def as_csv(self, path: str) -> None:\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\"train_loss,train_accuracy,val_loss,val_accuracy\\n\")\n",
    "            for train, val in zip(self.train, self.val):\n",
    "                f.write(f\"{train.loss},{train.accuracy},{val.loss},{val.accuracy}\\n\")\n",
    "\n",
    "\n",
    "class SlidingAverageTrainingMetrics:\n",
    "    n: int\n",
    "    _metrics: list[TrainingMetrics]\n",
    "\n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "        self._metrics = []\n",
    "\n",
    "    def add_metric(self, metric: TrainingMetrics):\n",
    "        self._metrics.append(metric)\n",
    "\n",
    "        if len(self._metrics) > self.n:\n",
    "            self._metrics.pop(0)\n",
    "\n",
    "    def get_average(self) -> TrainingMetrics:\n",
    "        return TrainingMetrics(\n",
    "            sum(m.loss for m in self._metrics) / len(self._metrics),\n",
    "            sum(m.accuracy for m in self._metrics) / len(self._metrics),\n",
    "        )\n",
    "\n",
    "\n",
    "class MaskedCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom loss function for scenarios where some classes are invalid for specific examples.\n",
    "\n",
    "    The loss ignores predictions made for invalid classes and normalizes the remaining\n",
    "    valid logits before computing cross-entropy.\n",
    "\n",
    "    Args:\n",
    "        reduction (str): Specifies the reduction to apply to the output:\n",
    "            'none' | 'mean' | 'sum'. Default: 'mean'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(\n",
    "        self, logits: torch.Tensor, targets: torch.Tensor, valid_mask: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits (torch.Tensor): Raw model output of shape [batch_size, num_classes]\n",
    "            targets (torch.Tensor): Ground truth labels of shape [batch_size]\n",
    "            valid_mask (torch.Tensor): Boolean mask of shape [batch_size, num_classes] where\n",
    "                                      True indicates a valid class and False indicates an invalid class\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss\n",
    "        \"\"\"\n",
    "\n",
    "        logits = torch.masked_fill(logits, ~valid_mask, 1e-9)\n",
    "\n",
    "        return F.cross_entropy(logits, targets, reduction=self.reduction)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    model: nn.Module\n",
    "    train_loader: DataLoader\n",
    "    val_loader: DataLoader\n",
    "    config: TrainingConfig\n",
    "\n",
    "    optimizer: torch.optim.Optimizer\n",
    "    scheduler: torch.optim.lr_scheduler.LambdaLR\n",
    "    criterion: MaskedCrossEntropyLoss\n",
    "\n",
    "    train_metrics: SlidingAverageTrainingMetrics\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        model: nn.Module,\n",
    "        train_dataset: ChessDataset,\n",
    "        val_dataset: ChessDataset,\n",
    "        config: TrainingConfig,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True, pin_memory=True\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, batch_size=config.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "        self.config = config\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=config.lr, weight_decay=config.weight_decay\n",
    "        )\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer,\n",
    "            lambda step: (\n",
    "                min(1.0, step / config.warmup_steps) if config.warmup_steps > 0 else 1.0\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.criterion = MaskedCrossEntropyLoss()\n",
    "\n",
    "        self.train_metrics = SlidingAverageTrainingMetrics(\n",
    "            config.training_loss_last_n_batches\n",
    "        )\n",
    "\n",
    "    def train(self) -> None:\n",
    "        best_val_loss = None\n",
    "        best_val_acc = None\n",
    "        global_step = 0\n",
    "        epoch = 0\n",
    "        history = TrainingHistory()\n",
    "\n",
    "        while True:\n",
    "            pbar = tqdm(self.train_loader, leave=True)\n",
    "\n",
    "            for boards, targets, legal_move_masks in pbar:\n",
    "                boards = boards.to(self.config.device)\n",
    "                targets = targets.to(self.config.device)\n",
    "                legal_move_masks = legal_move_masks.to(self.config.device)\n",
    "\n",
    "                metrics = self.train_batch(boards, targets, legal_move_masks)\n",
    "                self.train_metrics.add_metric(metrics)\n",
    "\n",
    "                # after `config.checkpoint_every` steps, compute validation metrics,\n",
    "                # save the best model, and add the training metrics to the history\n",
    "                if global_step > 0 and global_step % self.config.checkpoint_every == 0:\n",
    "                    val_metrics = self.compute_validation()\n",
    "                    history.add_point(self.train_metrics.get_average(), val_metrics)\n",
    "\n",
    "                    if best_val_loss is None or val_metrics.loss < best_val_loss:\n",
    "                        best_val_loss = val_metrics.loss\n",
    "                        best_val_acc = val_metrics.accuracy\n",
    "                        self.save_checkpoint()\n",
    "                    history.as_csv(self.config.metrics_path)\n",
    "\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"train_loss\": f\"{self.train_metrics.get_average().loss:.4f}\",\n",
    "                        \"train_acc\": f\"{self.train_metrics.get_average().accuracy:.4f}\",\n",
    "                        \"best_val_loss\": (\n",
    "                            f\"{best_val_loss:.4f}\"\n",
    "                            if best_val_loss is not None\n",
    "                            else \"N/A\"\n",
    "                        ),\n",
    "                        \"best_val_acc\": (\n",
    "                            f\"{best_val_acc:.4f}\"\n",
    "                            if best_val_acc is not None\n",
    "                            else \"N/A\"\n",
    "                        ),\n",
    "                        \"lr\": f\"{self.optimizer.param_groups[0]['lr']:.6f}\",\n",
    "                        \"global_step\": f\"{global_step:,}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "    def save_checkpoint(self) -> None:\n",
    "        with open(self.config.checkpoint_path, \"wb\") as f:\n",
    "            torch.save(self.model.state_dict(), f)\n",
    "\n",
    "    def train_batch(\n",
    "        self,\n",
    "        boards: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        legal_move_masks: torch.Tensor,\n",
    "    ) -> TrainingMetrics:\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        logits = self.model(boards)\n",
    "        loss = self.criterion(logits, targets, legal_move_masks)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            masked_logits = logits.masked_fill(legal_move_masks == 0, float(\"-inf\"))\n",
    "            _, predicted = torch.max(masked_logits, 1)\n",
    "            _, target_moves = torch.max(targets, 1)\n",
    "            correct = (predicted == target_moves).sum().item()\n",
    "            total = targets.size(0)\n",
    "            accuracy = correct / total\n",
    "\n",
    "        return TrainingMetrics(loss.item(), accuracy)\n",
    "\n",
    "    def compute_validation(self) -> TrainingMetrics:\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            for boards, targets, legal_move_masks in self.val_loader:\n",
    "                boards = boards.to(self.config.device)\n",
    "                targets = targets.to(self.config.device)\n",
    "                legal_move_masks = legal_move_masks.to(self.config.device)\n",
    "\n",
    "                logits = self.model(boards)\n",
    "                logits = logits.masked_fill(legal_move_masks == 0, float(\"-inf\"))\n",
    "                loss = self.criterion(logits, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                _, target_moves = torch.max(targets, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == target_moves).sum().item()\n",
    "\n",
    "            val_loss /= len(self.val_loader)\n",
    "            val_acc = correct / total\n",
    "\n",
    "            return TrainingMetrics(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 26/75 [00:07<00:14,  3.30it/s, train_loss=7.9192, train_acc=0.0583, best_val_loss=N/A, lr=0.000100, global_step=25]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m training_config \u001b[38;5;241m=\u001b[39m TrainingConfig(\n\u001b[1;32m      2\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[1;32m      3\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     device\u001b[38;5;241m=\u001b[39mget_device(),\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     17\u001b[0m     val_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     18\u001b[0m     config\u001b[38;5;241m=\u001b[39mtraining_config,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 170\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    168\u001b[0m legal_move_masks \u001b[38;5;241m=\u001b[39m legal_move_masks\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 170\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegal_move_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_metrics\u001b[38;5;241m.\u001b[39madd_metric(metrics)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# after `config.checkpoint_every` steps, compute validation metrics,\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# save the best model, and add the training metrics to the history\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 218\u001b[0m, in \u001b[0;36mTrainer.train_batch\u001b[0;34m(self, boards, targets, legal_move_masks)\u001b[0m\n\u001b[1;32m    216\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits, targets, legal_move_masks)\n\u001b[1;32m    217\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/ffgonext/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ffgonext/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ffgonext/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/ffgonext/lib/python3.10/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/ffgonext/lib/python3.10/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ffgonext/lib/python3.10/site-packages/torch/optim/adamw.py:466\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 466\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(\n",
    "    batch_size=12,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_steps=10,\n",
    "    checkpoint_path=\"checkpoint.pth\",\n",
    "    metrics_path=\"metrics.csv\",\n",
    "    checkpoint_every=1000,\n",
    "\n",
    "    training_loss_last_n_batches=10,\n",
    "    device=get_device(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    config=training_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffgonext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
