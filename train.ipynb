{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import chess\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_puzzle_accuracy(\n",
    "    model: ChessGPT_cls,\n",
    "    puzzle_data: pd.DataFrame,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy on chess puzzles.\n",
    "    A puzzle is considered solved if the intended first move is in the model's top 3 predictions.\n",
    "\n",
    "    Args:\n",
    "        model (ChessGPT): The trained model to evaluate\n",
    "        puzzle_data (pd.DataFrame): DataFrame containing puzzles with FEN positions and moves\n",
    "        tokenizer: Tokenizer object to convert FEN strings to board tokens\n",
    "        device (str): Device to run evaluation on\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy score between 0 and 1\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    pbar = tqdm(\n",
    "        puzzle_data.iterrows(), total=len(puzzle_data), desc=\"Evaluating puzzles\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, row in pbar:\n",
    "            # Get the FEN and first move from puzzle\n",
    "            fen = row[\"FEN\"]\n",
    "            moves = row[\"Moves\"].split()[0]  # Get first move only\n",
    "            intended_move = chess.Move.from_uci(moves)\n",
    "\n",
    "            # Create board from FEN\n",
    "            board = chess.Board(fen)\n",
    "\n",
    "            # Tokenize board\n",
    "            board_tokens = tokenize_board_v1(board)\n",
    "            board_tokens = torch.tensor(board_tokens, device=device).unsqueeze(\n",
    "                0\n",
    "            )  # Add batch dimension\n",
    "\n",
    "            # Get model's top moves\n",
    "            top_moves = model.get_top_moves(board_tokens, board, n=5)\n",
    "            predicted_moves = [move for move, _ in top_moves]\n",
    "\n",
    "            # Check if intended move is in top 3\n",
    "            if intended_move in predicted_moves:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            # Update progress bar with current accuracy\n",
    "            pbar.set_postfix({\"accuracy\": f\"{(correct/total):.2%}\"})\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nFinal puzzle accuracy: {accuracy:.2%}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: ChessGPT_cls,\n",
    "    train_dataset: ChessDataset,\n",
    "    val_dataset: ChessDataset | None = None,\n",
    "    *,\n",
    "    batch_size: int = 64,\n",
    "    epochs: int = 100,\n",
    "    learning_rate: float = 1e-4,\n",
    "    warmup_steps: int | None = None,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train the ChessGPT model on a dataset of chess positions and moves.\n",
    "\n",
    "    Args:\n",
    "        model (ChessGPT): The model to train\n",
    "        train_dataset (ChessDataset): Dataset containing training examples\n",
    "        val_dataset (ChessDataset, optional): Dataset for validation\n",
    "        batch_size (int, optional): Training batch size. Defaults to 32.\n",
    "        epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "        learning_rate (float, optional): Learning rate. Defaults to 1e-4.\n",
    "        warmup_steps (int, optional): Number of warmup steps. If provided, learning rate\n",
    "            will linearly increase from 0 to learning_rate over this many steps.\n",
    "        device (str, optional): Device to train on. Defaults to CUDA if available.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = None\n",
    "    if val_dataset is not None:\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    # Setup learning rate scheduler for warmup\n",
    "    scheduler = None\n",
    "    if warmup_steps is not None:\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lambda step: min(1.0, step / warmup_steps) if warmup_steps > 0 else 1.0,\n",
    "        )\n",
    "\n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    last_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Create progress bar for this epoch\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "\n",
    "        i = 0\n",
    "        for boards, targets in pbar:\n",
    "            boards = boards.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(boards)\n",
    "\n",
    "            # Compute loss (KL divergence between predicted and target distributions)\n",
    "            loss = criterion(F.log_softmax(logits, dim=-1), targets)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update learning rate for warmup\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Update running loss and progress bar\n",
    "            running_loss += loss.item()\n",
    "            i += 1\n",
    "            pbar.set_postfix({\"train_loss\": f\"{running_loss / i:.4f}\"})\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # Print current learning rate during warmup\n",
    "            if (\n",
    "                warmup_steps is not None\n",
    "                and global_step <= warmup_steps\n",
    "                and global_step % 100 == 0\n",
    "            ):\n",
    "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "                print(f\"Step {global_step}, LR: {current_lr:.6f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = None\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_steps = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for boards, targets in val_loader:\n",
    "                    boards = boards.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    logits = model(boards)\n",
    "                    loss = criterion(F.log_softmax(logits, dim=-1), targets)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_steps += 1\n",
    "\n",
    "            val_loss /= val_steps\n",
    "            print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "            with open(f\"info_{epoch}.txt\", \"w\") as fl:\n",
    "                fl.write(f\"train_loss: {running_loss/i}, val_loss: {val_loss}\")\n",
    "\n",
    "        if val_loss is None or last_val_loss is None or val_loss < last_val_loss:\n",
    "            last_val_loss = val_loss\n",
    "            print(f\"Saving model with validation loss {val_loss:.4f} as best model\")\n",
    "            torch.save(model.state_dict(), f\"model_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffgonext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
